= Data Lake Accelerator Deployment Guide
Think Big Analytics
May 2016

:toc:
:toclevels: 2
:toc-title: Contents

== About

This document explains how to install the Data Lake Accelerator framework as well as Elasticsearch, NiFi, and ActiveMQ. There are a few different ways you can
install it depending on whether or not you are installing all components on one edge node vs. multiple nodes.

== System Requirements

=== Dependencies

The Data Lake Accelerator services should be installed on an edge node.  The following should be available prior to the installing the Data Lake Starter.

.Dependencies
|===
|Redhat/GNU/Linux distributions
|RPM (for install)
|Java 1.7 (or greater)
|Hadoop 2.4+
|Spark 1.5.x+
|Apache NiFi 0.5+ (or Hortonworks DataFlow)
|Hive
|MySQL
|===

.Tested Platforms
|===
|Platform|URL|Version

|Hortonworks Sandbox|http://hortonworks.com/products/hortonworks-sandbox/| HDP 2.3, 2.4
|===

== Installation

=== Procedure for installing all components on one edge node

Follow the steps below to install the data lake accelerator. This procedure is also recommended for installing to a Hortonworks sandbox.


. Login to the host using root or sudo access

. R&D provides a pre-built RPM compiled against recent versions of Hadoop, Spark, and NiFi libraries.  Find and download the RPM file from artifactory and place on the
host linux machine. You can
right click the download:

           http://54.152.98.43:8080/artifactory/webapp/search/artifact/?7&q=thinkbig-datalake-accelerator  (requires VPN)

. Run RPM install

           $ rpm -ivh thinkbig-datalake-accelerator-<version>.noarch.rpm

. Run the setup wizard - sudo /opt/thinkbig/setup/setup-wizard.sh

    Follow the directions and it will install the following:
    * MySQL or Postgres scripts into the local database
    * Elasticsearch
    * ActiveMQ
    * NiFi and the Think Big dependencies

    Elasticsearch, NiFi, and ActiveMQ will be started when the wizard is finished

. Add nifi and thinkbig user to the HDFS supergroup or the group defined in hdfs-site.xml, for example:

    Hortonworks
    $ usermod -a -G hdfs nifi
    $ usermod -a -G hdfs thinkbig

    Cloudera
      $ groupadd supergroup
      # Add nifi and hdfs to that group:
      $ usermod -a -G supergroup nifi
      $ usermod -a -G supergroup hdfs

      Note: If you want to perform actions as a root user in a development environment run the below command
      $ usermod -a -G supergroup root

. Create a dropzone folder on the edge node for file ingest, for example:

    $ mkdir -p /var/dropzone
    $ chown nifi /var/dropzone

    Note: Files should be copied into the dropzone such that user nifi can read and remove.

. Complete this step for Cloudera installations ONLY

  See the appendix section below "Cloudera Configuration File Changes"

. Start the three Think Big services

           $ /opt/thinkbig/start-thinkbig-apps.sh

           At this point all services should be running. Note that services are started automatically on boot.

TIP: See section below on how to use the cleanup script to completely remove all of the components above. This is useful in
     a DEV or sandbox environment so you can run a clean install.

=== Procedure for installing each component manually

Follow the steps below to install the data lake accelerator manually. This method is useful if you are deploying products across multiple edge nodes


. For each step login to the the host using root or sudo access

. Find and download the RPM file from artifactory and place on the host linux machine you want to install the data lake
   accelerator services on. You can right click the download link and copy the url to use wget instead

           http://54.152.98.43:8080/artifactory/webapp/search/artifact/?7&q=thinkbig-datalake-accelerator  (requires VPN)

. Run data lake accelerator RPM install

           $ rpm -ivh thinkbig-datalake-accelerator-<version>.noarch.rpm

. Run the database scripts (see database configuration section)


. Install Elasticsearch

    You can leverage an existing elasticsearch installation or follow the steps in the elasticsearch
    script used by the wizard.

    /opt/thinkbig/setup/elasticsearch/install-elasticsearch.sh

. Install ActiveMQ

    You can leverage an existing ActiveMQ installation or follow the steps in the ActiveMQ script used by the wizard

    $ /opt/thinkbig/setup/activemq/install-activemq.sh

    NOTE: If installing on a different node than NiFi and thinkbig-services you will need to update the following properties

    $ /opt/nifi/ext-config/config.properties

        * spring.activemq.broker-url

    $ /opt/thinkbig/thinkbig-services/conf/application.properties

        * jms.activemq.broker.url


. Install NiFI

    You can leverage an existing NiFi installation or follow the steps in the setup directory which is used by
    the wizard. There are two steps:

    1. Install NiFi
    $ /opt/thinkbig/setup/nifi/install-nifi.sh

    2. Install Think Big specific components
    $ /opt/thinkbig/setup/nifi/install-thinkbig-components.sh

. Install Java 8

    If you already have Java 8 installed as the system version of Java then you can skip this step. However, if you are deploying NiFi or the Think Big RPM on a box with Java 7 then you will want to follow the below steps

    1. Install Java 8 in the /opt/java directory

    $ /opt/thinkbig/setup/java/install-java8.sh

    2. Update NiFi, thinkbig-ui, and thinkbig-services to use new Java version

    $ /opt/thinkbig/setup/java/change-java-home.sh

. Add nifi and thinkbig user to the HDFS supergroup or the group defined in hdfs-site.xml, for example:

    Hortonworks
    $ usermod -a -G hdfs nifi
    $ usermod -a -G hdfs thinkbig

    Cloudera
      $ groupadd supergroup
      # Add nifi and hdfs to that group:
      $ usermod -a -G supergroup nifi
      $ usermod -a -G supergroup hdfs

      Note: If you want to perform actions as a root user in a development environment run the below command
      $ usermod -a -G supergroup root

. Create a dropzone folder on the edge node for file ingest, for example:

    $ mkdir -p /var/dropzone
    $ chown nifi /var/dropzone

    Note: Files should be copied into the dropzone such that user nifi can read and remove.

. Complete this step for Cloudera installations ONLY

  See the appendix section below "Cloudera Configuration File Changes"

. Start the three Think Big services

           $ /opt/thinkbig/start-thinkbig-apps.sh

           At this point all services should be running

== Configuration

=== Configuration Files

Configuration for the data lake accelerator services are located under the following files:

    /opt/thinkbig/thinkbig-ui/conf/application.properties
    /opt/thinkbig/thinkbig-services/conf/application.properties


=== Database Setup

Data lake services can be configured to work with Postgres or MySQL. Database and permission setup scripts are provided to assist in the initial configuration process.   The script names relevant to setup are below:

==== MySQL
|===
|Script Name|Description
|/opt/thinkbig/setup/sql/mysql/setup-mysql.sh|Create tables used by data lake accelerator services
|/opt/thinkbig/setup/sql/mysql/drop-mysql.sh DROP|Used to remove the data lake accelerator schema(s)
|===


==== Postgres
TBD - Not yet supported


=== Optimizing Performance

You can adjust the memory setting for each services using the below environment variables

    export THINKBIG_UI_OPTS=Xmx4g
    export THINKBIG_SERVICES_OPTS=Xmx4g
    
The setting above would set the Java maximum heap size to 4 GB.    


== Starting the Services
Note: These below are installed as services and should start and stop automatically when the machine is rebooted

For starting and stopping the 3 data lake accelerator services there you can run the following scripts

   /opt/thinkbig/start-thinkbig-apps.sh
   /opt/thinkbig/stop-thinkbig-apps.sh

1. To Start individual services

    $ service activemq start
    $ service elasticsearch start
    $ service nifi start
    $ service thinkbig-spark-shell start
    $ service thinkbig-services start
    $ service thinkbig-ui start

2. To Stop individual services

    $ service activemq stop
    $ service elasticsearch stop
    $ service nifi stop
    $ service thinkbig-spark-shell stop
    $ service thinkbig-services stop
    $ service thinkbig-ui stop

3.  To get the status of individual services

    $ service activemq status
    $ service elasticsearch status
    $ service nifi status
    $ service thinkbig-spark-shell status
    $ service thinkbig-services status
    $ service thinkbig-ui status

== Log Output

=== Configuring Log Output

Log output for the services mentioned above are configured at:

			/opt/thinkbig/thinkbig-ui/conf/log4j.properties
			/opt/thinkbig/thinkbig-services/conf/log4j.properties

You may place logs where desired according to the 'log4j.appender.file.File' property.  Note the configuration line:

			log4j.appender.file.File=/var/log/<app>/<app>.log

=== Viewing Log Output

The default log locations for the various applications are located at:

/var/log/<service_name>

== Web and REST Access

Below are the default URL's and ports for the services

    Feed Manager and Operations UI
    http://127.0.0.1:8400
    username: dladmin
	password: thinkbig

    NiFi UI
    http://127.0.0.1:8079/nifi

    Elasticsearch REST API
    http://127.0.0.1:9200

    ActiveMQ Admin
    http://127.0.0.1:8161/admin

== Appendix: Cloudera Configuration File Changes

The configuration is setup to work out of the box with the Hortonworks sandbox. There are a few differences that require configuration changes for Cloudera.
    /opt/thinkbig/thinkbig-services/conf/application.properties

    .. Update the 3 MySQL password values to "cloudera"

    spring.datasource.password=cloudera
    metadata.datasource.password=cloudera
    hive.metastore.datasource.password=cloudera

    .. Update the hive username

    hive.datasource.username=hive

    .. Update the following parameters

    config.hive.schema=metastore
    nifi.executesparkjob.sparkhome=/usr/lib/spark

== Appendix: Cleanup script
For development and sandbox environments you can leverage the cleanup script to remove all of the Think Big services as well as Elasticsearch,
ActiveMQ, and NiFi.

    $ /opt/thinkbig/setup/dev/cleanup-env.sh

 IMPORTANT Only run this in a DEV environment. This will delete all application and the MySQL schema

== Appendix: Postgres Integration

TBD
	
