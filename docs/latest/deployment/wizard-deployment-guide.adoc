= Setup Wizard Deployment Guide

== About
Follow the steps below to install the data lake accelerator using the installation wizard script. This is convenient for local sandboxes (HDP/Cloudera)
and 1 node development boxes. The WGET command is used to download binaries so internet access is required.

NOTE: There is a Jira(PC-216) to create a script to generate an uber tar file with the RPM and all other binaries to avoid needing internet access

== Installation Locations

* Thinkbig Applications - /opt/thinkbig
* Java 8 - /opt/java/current
* NiFi - /opt/nifi/current
* ActiveMQ - RPM installation default location
* Elasticsearch - RPM installation default location

== Installation
For each step below you will need to login with root access

=== Install Think Big Services

. Find and download the RPM file from artifactory and place on the host linux machine you want to install the data lake
   accelerator services on. You can right click the download link and copy the url to use wget instead

           http://54.152.98.43:8080/artifactory/webapp/search/artifact/?7&q=thinkbig-datalake-accelerator  (requires VPN)

. Run data lake accelerator RPM install

           $ rpm -ivh thinkbig-datalake-accelerator-<version>.noarch.rpm

   NOTE: The RPM is hard coded at this time to install to /opt/thinkbig

. Run the setup wizard

    $ /opt/thinkbig/setup/setup-wizard.sh

    Follow the directions and it will install the following:
    * MySQL or Postgres scripts into the local database
    * Elasticsearch
    * ActiveMQ
    * Java 8 (If the system Java is 7 or below)
    * NiFi and the Think Big dependencies

    The Elasticsearch, NiFi, and ActiveMQ services will be started when the wizard is finished

. Add "nifi" and "thinkbig" user to the HDFS supergroup or the group defined in hdfs-site.xml, for example:

    Hortonworks
    $ usermod -a -G hdfs nifi
    $ usermod -a -G hdfs thinkbig

    Cloudera
      $ groupadd supergroup
      # Add nifi and hdfs to that group:
      $ usermod -a -G supergroup nifi
      $ usermod -a -G supergroup hdfs
      Optional: If you want to perform actions as a root user in a development environment run the below command
      $ usermod -a -G supergroup root

. Create a dropzone folder on the edge node for file ingest, for example:

    $ mkdir -p /var/dropzone
    $ chown nifi /var/dropzone

    Note: Files should be copied into the dropzone such that user nifi can read and remove.

. Complete this step for Cloudera installations ONLY

  See the appendix section below "Cloudera Configuration File Changes"

. Start the three Think Big services

           $ /opt/thinkbig/start-thinkbig-apps.sh

           At this point all services should be running. Note that services are started automatically on boot.

TIP: See section below on how to use the cleanup script to completely remove all of the components above. This is useful in
     a DEV or sandbox environment so you can run a clean install.

== Appendix: Cloudera Configuration File Changes

The configuration is setup to work out of the box with the Hortonworks sandbox. There are a few differences that require configuration changes for Cloudera.
    /opt/thinkbig/thinkbig-services/conf/application.properties

    .. Update the 3 MySQL password values to "cloudera"

    spring.datasource.password=cloudera
    metadata.datasource.password=cloudera
    hive.metastore.datasource.password=cloudera

    .. Update the Hive username

    hive.datasource.username=hive

    .. Update the Hive Metastore URL

    hive.metastore.datasource.url=jdbc:mysql://localhost:3306/metastore

    .. Update the following parameters

    config.hive.schema=metastore
    nifi.executesparkjob.sparkhome=/usr/lib/spark