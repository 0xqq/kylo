#
# Copyright (c) 2016.
#

# applicationDevOverride.properties
#
# How to use:  Add '--spring.config.location=classpath:/applicationDevOverride.properties' to program arguments

# Use alternate port that does not conflict with VMs
server.port=8284
##server.context-path=/pipeline-controller

jobs.bufferdata.remoteNotify.restServiceUrl=http://127.0.0.1:8284

#spring.datasource.url=jdbc:mysql://localhost:3307/pipeline_db
#spring.datasource.username=thinkbig
#spring.datasource.password=thinkbig
#spring.datasource.url=jdbc:mysql://localhost:3307/pipeline_db
#spring.datasource.username=thinkbig
#spring.datasource.password=thinkbig

spring.datasource.url=jdbc:mysql://localhost:3306/pipeline_db
spring.datasource.username=root

spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect

# Allows development of the DLS w/o Hadoop by skipping hive portions of job
jobs.bufferdata.transporter.fsConfig=LOCAL
jobs.hivedata.databaseName=SKIP_HIVE

job.aircraft.checkSchedule=0 * * * * *
#job.aircraft.checkSchedule=0 0 * * * *
application.mode=STANDALONE



#
# Prevents execution of jobs at startup.  Change to true, and the name of the job that should
# be run at startup if we want that behavior
#
spring.batch.job.enabled=false
spring.batch.job.names=


#
# Job configuration
#
jobs.bufferdata.packager.siteName=siteName

jobs.bufferdata.packager.outputFolder=/tmp/bufferserver/dropzone/.packager-output/feed=%FEED%
jobs.bufferdata.transporter.outputFolder=/tmp/bufferserver/transported/site=%SITE%/feed=%FEED%/job=%JOBUUID%/year=%YEAR%/month=%MONTH%/day=%DAY%/hour=%HOUR%
jobs.bufferdata.hive.shellArgs=%OUTPUT_FOLDER% %SITE% %STARTTIME% %JOBUUID% %DB_NAME% %SCHEMA_URL% 2>&1 | tee -a %LOG_FOLDER%/insertDataScript-'date +\%Y-\%m-\%d:\%H:\%M:\%S'.log
jobs.bufferdata.dq.shellArgs=%DB_NAME% %FEED% 2>&1 | tee -a %LOG_FOLDER%/checkDataScript-'date +\%Y-\%m-\%d:\%H:\%M:\%S'.log
jobs.hivedata.avroSchemaUrl=hdfs://sandbox.hortonworks.com:8020/schema/FileExtract.avsc
##jobs.hivedata.databaseName=faa_flights_db
jobs.hivedata.scriptLogFolder=/tmp/bufferserver/logs/hive

job.adsb.feed=ADSB
job.adsb.inputFolder=/tmp/bufferserver/dropzone/adsb
job.adsb.feedSchedule=0 0/1 * 1/1 * ? *
job.adsb.hiveShellScript=bin/ADSB/insertDataScript.sh
job.adsb.checkDataScript=bin/ADSB/checkDataScript.sh

job.aircraft.feed=Aircraft
job.aircraft.inputFolder=/tmp/bufferserver/dropzone/aircraft
job.aircraft.feedSchedule=30 0/1 * 1/1 * ? *
job.aircraft.hiveShellScript=bin/Aircraft/insertDataScript.sh
job.aircraft.checkDataScript=bin/Aircraft/checkDataScript.sh

job.master.feed=Master
job.master.inputFolder=/tmp/bufferserver/dropzone/master
job.master.feedSchedule=45 0/1 * 1/1 * ? *
job.master.hiveShellScript=plugin-bin/Master/insertDataScript.sh
job.master.checkDataScript=plugin-bin/Master/checkDataScript.sh

job.onTime.feed=OnTime
job.onTime.inputFolder=/tmp/bufferserver/dropzone/onTime
job.onTime.feedSchedule=15 0/1 * 1/1 * ? *
job.onTime.hiveShellScript=plugin-bin/OnTime/insertDataScript.sh
job.onTime.checkDataScript=plugin-bin/OnTime/checkDataScript.sh
job.onTime.checkSchedule:0 0/5 * 1/1 * ? *


job.randomFailure.feed=Random Failure
job.randomFailure.feedSchedule=45 0/1 * 1/1 * ? *


#
# Loom configuration
#
jobs.loomdata.serverVersion=2.4.0-rc5.718
jobs.loomdata.enabled=false
jobs.loomdata.serverBaseUrl=http://127.0.0.1:8083
jobs.loomdata.stopProcessingOnError=false

jobs.loomdata.username=loomuser
jobs.loomdata.password=thinkbig


