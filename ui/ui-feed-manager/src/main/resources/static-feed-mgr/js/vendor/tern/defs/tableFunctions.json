{
  "!name":"tableDataFunctions",
  "!define": {
    "Column": {
      "as": {
        "!type": "fn(alias: string) -> Column",
        "!doc": "Gives the column an alias.",
        "!spark": ".as(%s)",
        "!sparkType": "column"
      },
      "cast": {
        "!type": "fn(to: string) -> Column",
        "!doc": "Casts the column to a different type.",
        "!spark": ".cast(%s)",
        "!sparkType": "column"
      },
      "over": {
        "!type": "fn(window: WindowSpec) -> Column",
        "!doc": "Define a windowing column.",
        "!spark": ".over(%w)",
        "!sparkType": "column"
      }
    },
    "ConditionChain": {
      "otherwise": {
        "!type": "fn(value: Column) -> Column",
        "!doc": "Evaluates a list of conditions and returns one of multiple possible result expressions.",
        "!spark": ".otherwise(%c)",
        "!sparkType": "column"
      },
      "when": {
        "!type": "fn(condition: Column, value: Column) -> ConditionChain",
        "!doc": "Evaluates a list of conditions and returns one of multiple possible result expressions.",
        "!spark": ".when(%c, %c)",
        "!sparkType": "conditionchain"
      }
    },
    "GroupedData": {
      "agg": {
        "!type": "fn(expr1: Column, exprs_varargs: Column)",
        "!doc": "Compute aggregates by specifying a series of aggregate columns.",
        "!spark": ".agg(%c%,*c)",
        "!sparkType": "dataframe"
      },
      "avg": {
        "!type": "fn(colNames_varargs: string)",
        "!doc": "Compute the mean value for each numeric columns for each group.",
        "!spark": ".avg(%*s)",
        "!sparkType": "dataframe"
      },
      "count": {
        "!type": "fn()",
        "!doc": "Count the number of rows for each group.",
        "!spark": ".count()",
        "!sparkType": "dataframe"
      },
      "max": {
        "!type": "fn(colNames_varargs: string)",
        "!doc": "Compute the max value for each numeric columns for each group.",
        "!spark": ".max(%*s)",
        "!sparkType": "dataframe"
      },
      "mean": {
        "!type": "fn(colNames_varargs: string)",
        "!doc": "Compute the average value for each numeric columns for each group.",
        "!spark": ".mean(%*s)",
        "!sparkType": "dataframe"
      },
      "min": {
        "!type": "fn(colNames_varargs: string)",
        "!doc": "Compute the min value for each numeric column for each group.",
        "!spark": ".min(%*s)",
        "!sparkType": "dataframe"
      },
      "pivot": {
        "!type": "fn(pivotColumn: string) -> GroupedData",
        "!doc": "Pivots a column of the current DataFrame and perform the specified aggregation. (Spark 1.6)",
        "!spark": ".pivot(%s)",
        "!sparkType": "groupeddata"
      },
      "sum": {
        "!type": "fn(colNames_varargs: string)",
        "!doc": "Compute the sum for each numeric columns for each group.",
        "!spark": ".sum(%*s)",
        "!sparkType": "dataframe"
      }
    },
    "WindowSpec": {
      "orderBy": {
        "!type": "fn(cols_varargs: Column) -> WindowSpec",
        "!doc": "Defines the ordering columns in a WindowSpec.",
        "!spark": ".orderBy(%*c)",
        "!sparkType": "windowspec"
      },
      "partitionBy": {
        "!type": "fn(cols_varargs: Column) -> WindowSpec",
        "!doc": "Defines the partitioning columns in a WindowSpec.",
        "!spark": ".partitionBy(%*c)",
        "!sparkType": "windowspec"
      },
      "rangeBetween": {
        "!type": "fn(start: number, end: number) -> WindowSpec",
        "!doc": "Defines the frame boundaries, from start (inclusive) to end (inclusive).",
        "!spark": ".rangeBetween(%d, %d)",
        "!sparkType": "windowspec"
      },
      "rowsBetween": {
        "!type": "fn(start: number, end: number) -> WindowSpec",
        "!doc": "Defines the frame boundaries, from start (inclusive) to end (inclusive).",
        "!spark": ".rowsBetween(%d, %d)",
        "!sparkType": "windowspec"
      }
    }
  },

  "!AGGREGATE_FUNCTIONS": "Grouping and aggregating functions.",

  "approxCountDistinct": {
    "!type": "fn(e: Column, opt_rsd: number) -> Column",
    "!doc": "Aggregate function: returns the approximate number of distinct items in a group.",
    "!spark": "functions.approxCountDistinct(%c%,?f)",
    "!sparkType": "column"
  },
  "avg": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the average of the values in a group.",
    "!spark": "functions.avg(%c)",
    "!sparkType": "column"
  },
  "collect_list": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns a list of objects with duplicates. (Spark 1.6)",
    "!spark": "functions.collect_list(%c)",
    "!sparkType": "column"
  },
  "collect_set": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns a set of objects with duplicate elements eliminated. (Spark 1.6)",
    "!spark": "functions.collect_set(%c)",
    "!sparkType": "column"
  },
  "corr": {
    "!type": "fn(column1: Column, column2: Column) -> Column",
    "!doc": "Aggregate function: returns the Pearson Correlation Coefficient for two columns. (Spark 1.6)",
    "!spark": "functions.corr(%c, %c)",
    "!sparkType": "column"
  },
  "count": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the number of items in a group.",
    "!spark": "functions.count(%c)",
    "!sparkType": "column"
  },
  "countDistinct": {
    "!type": "fn(expr: Column, exprs_varargs: Column) -> Column",
    "!doc": "Aggregate function: returns the number of distinct items in a group.",
    "!spark": "functions.countDistinct(%c%,*c)",
    "!sparkType": "column"
  },
  "first": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the first value in a group.",
    "!spark": "functions.first(%c)",
    "!sparkType": "column"
  },
  "kurtosis": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the kurtosis of the values in a group. (Spark 1.6)",
    "!spark": "functions.kurtosis(%c)",
    "!sparkType": "column"
  },
  "last": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the last value in a group.",
    "!spark": "functions.last(%c)",
    "!sparkType": "column"
  },
  "max": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the maximum value of the expression in the group.",
    "!spark": "functions.max(%c)",
    "!sparkType": "column"
  },
  "mean": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the average of the values in a group.",
    "!spark": "functions.mean(%c)",
    "!sparkType": "column"
  },
  "min": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the minimum value of the expression in a group.",
    "!spark": "functions.min(%c)",
    "!sparkType": "column"
  },
  "skewness": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the skewness of the values in a group. (Spark 1.6)",
    "!spark": "functions.skewness(%c)",
    "!sparkType": "column"
  },
  "stddev": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: alias for stddev_samp. (Spark 1.6)",
    "!spark": "functions.stddev(%c)",
    "!sparkType": "column"
  },
  "stddev_pop": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the population standard deviation of the expression in a group. (Spark 1.6)",
    "!spark": "functions.stddev_pop(%c)",
    "!sparkType": "column"
  },
  "stddev_samp": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the sample standard deviation of the expression in a group. (Spark 1.6)",
    "!spark": "functions.stddev_samp(%c)",
    "!sparkType": "column"
  },
  "sum": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the sum of all values.",
    "!spark": "functions.sum(%c)",
    "!sparkType": "column"
  },
  "sumDistinct": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the sum of distinct values in the expression.",
    "!spark": "functions.sumDistinct(%c)",
    "!sparkType": "column"
  },
  "var_pop": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the population variance of the values in a group. (Spark 1.6)",
    "!spark": "functions.var_pop(%c)",
    "!sparkType": "column"
  },
  "var_samp": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: returns the unbiased variance of the values in a group. (Spark 1.6)",
    "!spark": "functions.var_samp(%c)",
    "!sparkType": "column"
  },
  "variance": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Aggregate function: alias for var_samp.",
    "!spark": "functions.variance(%c)",
    "!sparkType": "column"
  },

  "!COLLECTION_FUNCTIONS": "Functions for collections.",

  "array_contains": {
    "!type": "fn(column: Column, value: Column) -> Column",
    "!doc": "Returns true if the array contain the value.",
    "!spark": "functions.array_contains(%c, %c)",
    "!sparkType": "column"
  },
  "explode": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Creates a new row for each element in the given array or map column.",
    "!spark": "functions.explode(%c)",
    "!sparkType": "column"
  },
  "get_json_object": {
    "!type": "fn(e: Column, path: string) -> Column",
    "!doc": "Extracts json object from a json string based on json path specified, and returns json string of the extracted json object.",
    "!spark": "functions.get_json_object(%c, %s)",
    "!sparkType": "column"
  },
  "json_tuple": {
    "!type": "fn(json: Column, fields_varargs: string) -> Column",
    "!doc": "Creates a new row for a json column according to the given field names.",
    "!spark": "functions.json_tuple(%c%,*s)",
    "!sparkType": "column"
  },
  "size": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Returns length of array or map.",
    "!spark": "functions.size(%c)",
    "!sparkType": "column"
  },
  "sort_array": {
    "!type": "fn(e: Column, opt_asc: boolean) -> Column",
    "!doc": "Sorts the input array for the given column in ascending / descending order, according to the natural ordering of the array elements.",
    "!spark": "functions.sort_array(%c%,?b)",
    "!sparkType": "column"
  },

  "!DATE_TIME_FUNCTIONS": "Functions for date/time columns.",

  "add_months": {
    "!type": "fn(startDate: Column, numMonths: number) -> Column",
    "!doc": "Returns the date that is numMonths after startDate.",
    "!spark": "functions.add_months(%c, %d)",
    "!sparkType": "column"
  },
  "current_date": {
    "!type": "fn() -> Column",
    "!doc": "Returns the current date as a date column.",
    "!spark": "functions.current_date()",
    "!sparkType": "column"
  },
  "current_timestamp": {
    "!type": "fn() -> Column",
    "!doc": "Returns the current timestamp as a timestamp column.",
    "!spark": "functions.current_timestamp()",
    "!sparkType": "column"
  },
  "date_add": {
    "!type": "fn(start: Column, days: number) -> Column",
    "!doc": "Returns the date that is days days after start.",
    "!spark": "functions.date_add(%c, %d)",
    "!sparkType": "column"
  },
  "date_format": {
    "!type": "fn(dateExpr: Column, format: string) -> Column",
    "!doc": "Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.",
    "!spark": "functions.date_format(%c, %s)",
    "!sparkType": "column"
  },
  "date_sub": {
    "!type": "fn(start: Column, days: number) -> Column",
    "!doc": "Returns the date that is days days before start.",
    "!spark": "functions.date_sub(%c, %d)",
    "!sparkType": "column"
  },
  "datediff": {
    "!type": "fn(end: Column, start: Column) -> Column",
    "!doc": "Returns the number of days from start to end.",
    "!spark": "functions.datediff(%c, %c)",
    "!sparkType": "column"
  },
  "dayofmonth": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the day of the month as an integer from a given date/timestamp/string.",
    "!spark": "functions.dayofmonth(%c)",
    "!sparkType": "column"
  },
  "dayofyear": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the day of the year as an integer from a given date/timestamp/string.",
    "!spark": "functions.dayofyear(%c)",
    "!sparkType": "column"
  },
  "from_unixtime": {
    "!type": "fn(ut: Column, f: string) -> Column",
    "!doc": "Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.",
    "!spark": "functions.from_unixtime(%c, %s)",
    "!sparkType": "column"
  },
  "from_utc_timestamp": {
    "!type": "fn(ts: Column, tz: string) -> Column",
    "!doc": "Assumes given timestamp is UTC and converts to given timezone.",
    "!spark": "functions.from_utc_timestamp(%c, %s)",
    "!sparkType": "column"
  },
  "hour": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the hours as an integer from a given date/timestamp/string.",
    "!spark": "functions.hour(%c)",
    "!sparkType": "column"
  },
  "last_day": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Given a date column, returns the last day of the month which the given date belongs to.",
    "!spark": "functions.last_day(%c)",
    "!sparkType": "column"
  },
  "minute": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the minutes as an integer from a given date/timestamp/string.",
    "!spark": "functions.minute(%c)",
    "!sparkType": "column"
  },
  "month": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the month as an integer from a given date/timestamp/string.",
    "!spark": "functions.month(%c)",
    "!sparkType": "column"
  },
  "next_day": {
    "!type": "fn(e: Column, dayOfWeek: string) -> Column",
    "!doc": "Given a date column, returns the first date which is later than the value of the date column that is on the specified day of the week.",
    "!spark": "functions.next_day(%c, %s)",
    "!sparkType": "column"
  },
  "quarter": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the quarter as an integer from a given date/timestamp/string.",
    "!spark": "functions.quarter(%c)",
    "!sparkType": "column"
  },
  "second": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the seconds as an integer from a given date/timestamp/string.",
    "!spark": "functions.second(%c)",
    "!sparkType": "column"
  },
  "to_date": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Converts the column into DateType.",
    "!spark": "functions.to_date(%c)",
    "!sparkType": "column"
  },
  "to_utc_timestamp": {
    "!type": "fn(ts: Column, tz: string) -> Column",
    "!doc": "Assumes given timestamp is in given timezone and converts to UTC.",
    "!spark": "functions.to_utc_timestamp(%c, %s)",
    "!sparkType": "column"
  },
  "trunc": {
    "!type": "fn(date: Column, format: string) -> Column",
    "!doc": "Returns date truncated to the unit specified by the format.",
    "!spark": "functions.trunc(%c, %s)",
    "!sparkType": "column"
  },
  "unix_timestamp": {
    "!type": "fn(opt_s: Column, opt_p: string) -> Column",
    "!doc": "Convert time string with given pattern.",
    "!spark": "functions.unix_timestamp(%?c%,?s)",
    "!sparkType": "column"
  },
  "weekofyear": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the week number as an integer from a given date/timestamp/string.",
    "!spark": "functions.weekofyear(%c)",
    "!sparkType": "column"
  },
  "year": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Extracts the year as an integer from a given date/timestamp/string.",
    "!spark": "functions.year(%c)",
    "!sparkType": "column"
  },

  "!MATH_FUNCTIONS": "These functions define mathematical operations on columns.",

  "acos": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the cosine inverse.",
    "!spark": "functions.acos(%c)",
    "!sparkType": "column"
  },
  "asin": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the sine inverse.",
    "!spark": "functions.asin(%c)",
    "!sparkType": "column"
  },
  "atan": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the tangent inverse.",
    "!spark": "functions.atan(%c)",
    "!sparkType": "column"
  },
  "atan2": {
    "!type": "fn(l: Column, r: Column) -> Column",
    "!doc": "Returns the angle theta from the conversion of rectangular coordinates to polar.",
    "!spark": "functions.atan2(%c, %c)",
    "!sparkType": "column"
  },
  "bin": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "An expression that returns the string representation of the binary value of the given long column.",
    "!spark": "functions.bin(e)",
    "!sparkType": "column"
  },
  "cbrt": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the cube-root of the given value.",
    "!spark": "functions.cbrt(%c)",
    "!sparkType": "column"
  },
  "ceil": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the ceiling of the given value.",
    "!spark": "functions.ceil(%c)",
    "!sparkType": "column"
  },
  "conv": {
    "!type": "fn(num: Column, fromBase: number, toBase: number) -> Column",
    "!doc": "Convert a number in a string column from one base to another.",
    "!spark": "functions.conv(%c, %n, %n)",
    "!sparkType": "column"
  },
  "cos": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the cosine of the given value.",
    "!spark": "functions.cos(%c)",
    "!sparkType": "column"
  },
  "cosh": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the hyerbolic cosine of the given value.",
    "!spark": "functions.cosh(%c)",
    "!sparkType": "column"
  },
  "exp": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the exponential of the given value.",
    "!spark": "functions.exp(%c)",
    "!sparkType": "column"
  },
  "expm1": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the exponential of the given value minus one.",
    "!spark": "functions.expm1(%c)",
    "!sparkType": "column"
  },
  "factorial": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the factorial of the given value.",
    "!spark": "functions.factorial(%c)",
    "!sparkType": "column"
  },
  "floor": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the floor of the given value.",
    "!spark": "functions.floor(%c)",
    "!sparkType": "column"
  },
  "hex": {
    "!type": "fn(column: Column) -> Column",
    "!doc": "Computes hex value of the given column.",
    "!spark": "functions.hex(%c)",
    "!sparkType": "column"
  },
  "hypot": {
    "!type": "fn(l: Column, r: Column) -> Column",
    "!doc": "Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow.",
    "!spark": "functions.hypot(%c, %c)",
    "!sparkType": "column"
  },
  "ln": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the natural logarithm of the given value.",
    "!spark": "functions.log(%c)",
    "!sparkType": "column"
  },
  "log": {
    "!type": "fn(base: number, a: Column) -> Column",
    "!doc": "Returns the first argument-base logarithm of the second argument.",
    "!spark": "functions.log(%f, %c)",
    "!sparkType": "column"
  },
  "log10": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the logarithm of the given value in Base 10.",
    "!spark": "functions.log10(%c)",
    "!sparkType": "column"
  },
  "log1p": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the natural logarithm of the given value plus one.",
    "!spark": "functions.log1p(%c)",
    "!sparkType": "column"
  },
  "log2": {
    "!type": "fn(expr: Column) -> Column",
    "!doc": "Computes the logarithm of the given column in base 2.",
    "!spark": "functions.log2(%c)",
    "!sparkType": "column"
  },
  "pmod": {
    "!type": "fn(dividend: Column, divisor: Column) -> Column",
    "!doc": "Returns the positive value of dividend mod divisor.",
    "!spark": "functions.pmod(%c, %c)",
    "!sparkType": "column"
  },
  "pow": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Returns the value of the first argument raised to the power of the second.",
    "!spark": "functions.pow(%c, %c)",
    "!sparkType": "column"
  },
  "rint": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Returns the double value that is closest in value to the argument and is equal to a mathematical integer.",
    "!spark": "functions.rint(%c)",
    "!sparkType": "column"
  },
  "round": {
    "!type": "fn(e: Column, scale: number) -> Column",
    "!doc": "Round the value of e to scale decimal places if scale >= 0 or at integral part when scale < 0.",
    "!spark": "functions.round(%c, %d)",
    "!sparkType": "column"
  },
  "shiftLeft": {
    "!type": "fn(e: Column, numBits: number) -> Column",
    "!doc": "Shift the the given value numBits left.",
    "!spark": "functions.shiftLeft(%c, %d)",
    "!sparkType": "column"
  },
  "shiftRight": {
    "!type": "fn(e: Column, numBits: number) -> Column",
    "!doc": "Shift the the given value numBits right.",
    "!spark": "functions.shiftRight(%c, %d)",
    "!sparkType": "column"
  },
  "shiftRightUnsigned": {
    "!type": "fn(e: Column, numBits: number) -> Column",
    "!doc": "Unsigned shift the the given value numBits right.",
    "!spark": "functions.shiftRightUnsigned(%c, %d)",
    "!sparkType": "column"
  },
  "signum": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the signum of the given value.",
    "!spark": "functions.signum(%c)",
    "!sparkType": "column"
  },
  "sin": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the sine of the given value.",
    "!spark": "functions.sin(%c)",
    "!sparkType": "column"
  },
  "sinh": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the hyperbolic sine of the given value.",
    "!spark": "functions.sinh(%c)",
    "!sparkType": "column"
  },
  "sqrt": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the square root of the specified float value.",
    "!spark": "functions.sqrt(%c)",
    "!sparkType": "colum"
  },
  "tan": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the tangent of the given value.",
    "!spark": "functions.tan(%c)",
    "!sparkType": "column"
  },
  "tanh": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the hyperbolic tangent of the given value.",
    "!spark": "fn(e: Column) -> Column",
    "!sparkType": "column"
  },
  "toDegrees": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Converts an angle measured in radians to an approximately equivalent angle measured in degrees.",
    "!spark": "functions.toDegrees(%c)",
    "!sparkType": "column"
  },
  "toRadians": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Converts an angle measured in degrees to an approximately equivalent angle measured in radians.",
    "!spark": "functions.toRadians(%c)",
    "!sparkType": "column"
  },
  "unhex": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Inverse of hex.",
    "!spark": "functions.unhex(%c)",
    "!sparkType": "column"
  },

  "!MISC_FUNCTIONS": "Miscellaneous functions.",

  "crc32": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Calculates the cyclic redundancy check value (CRC32) of a binary column and returns the value as a bigint.",
    "!spark": "functions.crc32(%c)",
    "!sparkType": "column"
  },
  "md5": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Calculates the MD5 digest of a binary column and returns the value as a 32 character hex string.",
    "!spark": "functions.md5(%c)",
    "!sparkType": "column"
  },
  "sha1": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Calculates the SHA-1 digest of a binary column and returns the value as a 40 character hex string.",
    "!spark": "functions.sha1(%c)",
    "!sparkType": "column"
  },
  "sha2": {
    "!type": "fn(e: Column, numBits: number) -> Column",
    "!doc": "Calculates the SHA-2 family of hash functions of a binary column and returns the value as a hex string.",
    "!spark": "functions.sha2(%c)",
    "!sparkType": "column"
  },

  "!NON_AGGREGATE_FUNCTIONS": "Non-aggregate functions.",

  "abs": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the absolute value.",
    "!spark": "functions.abs(%c)",
    "!sparkType": "column"
  },
  "array": {
    "!type": "fn(cols_varargs: Column) -> Column",
    "!doc": "Creates a new array column.",
    "!spark": "functions.array(%*c)",
    "!sparkType": "column"
  },
  "bitwiseNot": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes bitwise NOT.",
    "!spark": "functions.bitwiseNOT(%c)",
    "!sparkType": "column"
  },
  "coalesce": {
    "!type": "fn(e_varargs: Column) -> Column",
    "!doc": "Returns the first column that is not null.",
    "!spark": "functions.coalesce(%*c)",
    "!sparkType": "column"
  },
  "expr": {
    "!type": "fn(expr: string) -> Column",
    "!doc": "Parses the expression string into the column that it represents, similar to DataFrame.",
    "!spark": "functions.expr(%s)",
    "!sparkType": "column"
  },
  "greatest": {
    "!type": "fn(exprs_varargs: Column) -> Column",
    "!doc": "Returns the greatest value of the list of values, skipping null values.",
    "!spark": "functions.greatest(%*c)",
    "!sparkType": "column"
  },
  "input_file_name": {
    "!type": "fn() -> Column",
    "!doc": "Creates a string column for the file name of the current Spark task. (Spark 1.6)",
    "!spark": "functions.input_file_name()",
    "!sparkType": "column"
  },
  "isnan": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Return true iff the column is NaN. (Spark 1.6)",
    "!spark": "functions.isnan(%c)",
    "!sparkType": "column"
  },
  "isnull": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Return true iff the column is null.",
    "!spark": "functions.isnull(%c)",
    "!sparkType": "column"
  },
  "least": {
    "!type": "fn(exprs_varargs: Column) -> Column",
    "!doc": "Returns the least value of the list of values, skipping null values.",
    "!spark": "functions.least(%*c)",
    "!sparkType": "column"
  },
  "monotonically_increasing_id": {
    "!type": "fn() -> Column",
    "!doc": "Generates monotonically increasing 64-bit integers.",
    "!spark": "functions.monotonically_increasing_id()",
    "!sparkType": "column"
  },
  "nanv1": {
    "!type": "fn(col1: Column, col2: Column) -> Column",
    "!doc": "Returns col1 if it is not NaN, or col2 if col1 is NaN.",
    "!spark": "functions.nanv1(%c, %c)",
    "!sparkType": "column"
  },
  "negate": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Unary minus.",
    "!spark": "functions.negate(%c)",
    "!sparkType": "column"
  },
  "not": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Inversion of boolean expression.",
    "!spark": "functions.not(%c)",
    "!sparkType": "column"
  },
  "rand": {
    "!type": "fn(opt_seed: number) -> Column",
    "!doc": "Generate a random column with i.i.d. samples from U[0.0, 1.0].",
    "!spark": "functions.rand(%?d)",
    "!sparkType": "column"
  },
  "randn": {
    "!type": "fn(opt_seed: number) -> Column",
    "!doc": "Generate a column with i.i.d. samples from the standard normal distribution.",
    "!spark": "functions.randn(%?d)",
    "!sparkType": "column"
  },
  "spark_partition_id": {
    "!type": "fn()",
    "!doc": "Partition ID of the Spark task. (Spark 1.6)",
    "!spark": "functions.spark_partition_id()",
    "!sparkType": "column"
  },
  "struct": {
    "!type": "fn(cols_varargs: Column) -> Column",
    "!doc": "Creates a new struct column.",
    "!spark": "functions.struct(%*c)",
    "!sparkType": "column"
  },
  "when": {
    "!type": "fn(condition: Column, value: Column) -> ConditionChain",
    "!doc": "Evaluates a list of conditions and returns one of multiple values.",
    "!spark": "functions.when(%c, %c)",
    "!sparkType": "conditionchain"
  },

  "!SORTING_FUNCTIONS": "Functions for sorting.",

  "asc": {
    "!type": "fn(columnName: string) -> Column",
    "!doc": "Returns a sort expression based on ascending order of the column.",
    "!spark": "functions.asc(%c)",
    "!sparkType": "column"
  },
  "desc": {
    "!type": "fn(columnName: string) -> Column",
    "!doc": "Returns a sort expression based on descending order of the column.",
    "!spark": "functions.desc(%c)",
    "!sparkType": "column"
  },

  "!STRING_FUNCTIONS": "These functions define operations on string columns.",

  "ascii": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the numeric value of the first character of the string column, and returns the result as a int column.",
    "!spark": "functions.ascii(%c)",
    "!sparkType": "column"
  },
  "base64": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the BASE64 encoding of a binary column and returns it as a string column.",
    "!spark": "functions.base64(%c)",
    "!sparkType": "column"
  },
  "concat": {
    "!type": "fn(exprs_varargs: Column) -> Column",
    "!doc": "Concatenates multiple input string columns together into a single string column.",
    "!spark": "functions.concat(%*c)",
    "!sparkType": "column"
  },
  "concat_ws": {
    "!type": "fn(sep: string, exprs_varargs: Column) -> Column",
    "!doc": "Concatenates multiple input string columns together into a single string column, using the given separator.",
    "!spark": "functions.concat_ws(%s%,*c)",
    "!sparkType": "column"
  },
  "decode": {
    "!type": "fn(value: Column, charset: string) -> Column",
    "!doc": "Computes the first argument into a string from a binary using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').",
    "!spark": "functions.decode(%c, %s)",
    "!sparkType": "column"
  },
  "encode": {
    "!type": "fn(value: Column, charset: string) -> Column",
    "!doc": "Computes the first argument into a binary from a string using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').",
    "!spark": "functions.encode(%c, %s)",
    "!sparkType": "column"
  },
  "format_number": {
    "!type": "fn(x: Column, d: number) -> Column",
    "!doc": "Formats numeric column x to a format like '#,###,###.##', rounded to d decimal places, and returns the result as a string column.",
    "!spark": "functions.format_number(%c, %d)",
    "!sparkType": "column"
  },
  "format_string": {
    "!type": "fn(format: string, arguments_varargs: Column) -> Column",
    "!doc": "Formats the arguments in printf-style and returns the result as a string column.",
    "!spark": "functions.format_string(%c%,*c)",
    "!sparkType": "column"
  },
  "initcap": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Returns a new string column by converting the first letter of each word to uppercase.",
    "!spark": "functions.initcap(%c)",
    "!sparkType": "column"
  },
  "instr": {
    "!type": "fn(str: Column, substring: string) -> Column",
    "!doc": "Locate the position of the first occurrence of substr column in the given string.",
    "!spark": "functions.instr(%c)",
    "!sparkType": "column"
  },
  "length": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Computes the length of a given string or binary column.",
    "!spark": "functions.length(%c)",
    "!sparkType": "column"
  },
  "levenshtein": {
    "!type": "fn(l: Column, r: Column) -> Column",
    "!doc": "Computes the Levenshtein distance of the two given string columns.",
    "!spark": "functions.levenshtein(%c)",
    "!sparkType": "column"
  },
  "locate": {
    "!type": "fn(substr: string, str: Column, opt_pos: number) -> Column",
    "!doc": "Locate the position of the first occurrence of substr in a string column, after position pos.",
    "!spark": "functions.locate(%s, %c%,?d)",
    "!sparkType": "column"
  },
  "lower": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Converts a string column to lower case.",
    "!spark": "functions.lower(%c)",
    "!sparkType": "column"
  },
  "lpad": {
    "!type": "fn(str: Column, len: number, pad: string) -> Column",
    "!doc": "Left-pad the string column with pad to a length of len.",
    "!spark": "functions.lpad(%c, %d, %s)",
    "!sparkType": "column"
  },
  "ltrim": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Trim the spaces from left end for the specified string value.",
    "!spark": "functions.ltrim(%c)",
    "!sparkType": "column"
  },
  "regexp_extract": {
    "!type": "fn(e: Column, exp: string, groupIdx: number) -> Column",
    "!doc": "Extract a specific(idx) group identified by a java regex, from the specified string column.",
    "!spark": "functions.regexp_extract(%c, %s, %d)",
    "!sparkType": "column"
  },
  "regexp_replace": {
    "!type": "fn(e: Column, pattern: string, replacement: string) -> Column",
    "!doc": "Replace all substrings of the specified string value that match regexp with rep.",
    "!spark": "functions.regexp_replace(%c, %s, %s)",
    "!sparkType": "column"
  },
  "repeat": {
    "!type": "fn(str: Column, n: number) -> Column",
    "!doc": "Repeats a string column n times, and returns it as a new string column.",
    "!spark": "functions.repeat(%c, %d)",
    "!sparkType": "column"
  },
  "reverse": {
    "!type": "fn(str: Column) -> Column",
    "!doc": "Reverses the string column and returns it as a new string column.",
    "!spark": "functions.reverse(%c)",
    "!sparkType": "column"
  },
  "rpad": {
    "!type": "fn(str: Column, len: number, pad: string) -> Column",
    "!doc": "Right-padded with pad to a length of len.",
    "!spark": "functions.rpad(%c, %d, %s)",
    "!sparkType": "column"
  },
  "rtrim": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Trim the spaces from right end for the specified string value.",
    "!spark": "functions.rtrim(%c)",
    "!sparkType": "column"
  },
  "soundex": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Return the soundex code for the specified expression.",
    "!spark": "functions.soundex(%c)",
    "!sparkType": "column"
  },
  "split": {
    "!type": "fn(str: Column, pattern: string) -> Column",
    "!doc": "Splits str around pattern (pattern is a regular expression).",
    "!spark": "functions.split(%c, %s)",
    "!sparkType": "column"
  },
  "substring": {
    "!type": "fn(str: Column, pos: number, len: number) -> Column",
    "!doc": "Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type.",
    "!spark": "functions.substring(%c, %d, %d)",
    "!sparkType": "column"
  },
  "substring_index": {
    "!type": "fn(str: Column, delim: string, count: number) -> Column",
    "!doc": "Returns the substring from string str before count occurrences of the delimiter delim.",
    "!spark": "functions.substring_index(%c, %s, %d)",
    "!sparkType": "column"
  },
  "translate": {
    "!type": "fn(src: Column, matchingString: string, replaceString: string) -> Column",
    "!doc": "Translate any character in the src by a character in replaceString.",
    "!spark": "functions.translate(%c, %s, %s)",
    "!sparkType": "column"
  },
  "trim": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Trim the spaces from both ends for the specified string column.",
    "!spark": "functions.trim(%c)",
    "!sparkType": "column"
  },
  "unbase64": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Decodes a BASE64 encoded string column and returns it as a binary column.",
    "!spark": "functions.unbase64(%c)",
    "!sparkType": "column"
  },
  "upper": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "Converts a string column to upper case.",
    "!spark": "functions.upper(%c)",
    "!sparkType": "column"
  },

  "!WINDOW_FUNCTIONS": "Functions over windows.",

  "cume_dist": {
    "!type": "fn() -> Column",
    "!doc": "Window function: returns the cumulative distribution of values within a window partition. (Spark 1.6)",
    "!spark": "functions.cume_dist()",
    "!sparkType": "column"
  },
  "dense_rank": {
    "!type": "fn() -> Column",
    "!doc": "Window function: returns the rank of rows within a window partition, without any gaps. (Spark 1.6)",
    "!spark": "functions.dense_rank()",
    "!sparkType": "column"
  },
  "lag": {
    "!type": "fn(e: Column, offset: number, opt_defaultValue: Column) -> Column",
    "!doc": "Window function: returns the value that is offset rows before the current row, and defaultValue if there is less than offset rows before the current row.",
    "!spark": "functions.lag(%c, %d%,?c)",
    "!sparkType": "column"
  },
  "lead": {
    "!type": "fn(e: Column, offset: number, opt_defaultValue: Column) -> Column",
    "!doc": "Window function: returns the value that is offset rows after the current row, and defaultValue if there is less than offset rows after the current row.",
    "!spark": "functions.lead(%c, %d%,?c)",
    "!sparkType": "column"
  },
  "ntile": {
    "!type": "fn(n: number) -> Column",
    "!doc": "Window function: returns the ntile group id (from 1 to n inclusive) in an ordered window partition.",
    "!spark": "functions.ntile(%d)",
    "!sparkType": "column"
  },
  "percent_rank": {
    "!type": "fn()",
    "!doc": "Window function: returns the relative rank (i.e. percentile) of rows within a window partition. (Spark 1.6)",
    "!spark": "functions.percent_rank(%n)",
    "!sparkType": "column"
  },
  "rank": {
    "!type": "fn()",
    "!doc": "Window function: returns the rank of rows within a window partition.",
    "!spark": "functions.rank()",
    "!sparkType": "column"
  },
  "row_number": {
    "!type": "fn()",
    "!doc": "Window function: returns a sequential number starting at 1 within a window partition. (Spark 1.6)",
    "!spark": "functions.row_number()",
    "!sparkType": "column"
  },

  "!UNGROUPED": "Functions not assigned to a group.",

  "months_between": {
    "!type": "fn(date1: Column, date2: Column) -> Column",
    "!doc": "Number of months between date1 and date2.",
    "!spark": "functions.months_between(%c, %c)",
    "!sparkType": "column"
  },

  "!COLUMN_EXPRESSION_OPERATORS": "Functions for Column objects.",

  "add": {
    "!type": "fn(col1: Column, col2: Column) -> Column",
    "!doc": "Add two numbers together.",
    "!spark": "%c.plus(%c)",
    "!sparkType": "column"
  },
  "and": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Boolean AND.",
    "!spark": "%c.and(%c)",
    "!sparkType": "column"
  },
  "between": {
    "!type": "fn(e: Column, lowerBound: Column, upperBound: Column) -> Column",
    "!doc": "True if column is between the lower bound and upper bound, inclusive",
    "!spark": "%c.between(%c, %c)",
    "!sparkType": "column"
  },
  "bitwiseAnd": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Compute bitwise AND of two expressions.",
    "!spark": "%c.bitwiseAND(%c)",
    "!sparkType": "column"
  },
  "bitwiseOr": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Compute bitwise OR of two expressions.",
    "!spark": "%c.bitwiseOR(%c)",
    "!sparkType": "column"
  },
  "bitwiseXor": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Compute bitwise XOR of two expressions.",
    "!spark": "%c.bitwiseXOR(%c)",
    "!sparkType": "column"
  },
  "contains": {
    "!type": "fn(e: Column, other: Column) -> Column",
    "!doc": "Contains the other column.",
    "!spark": "%c.contains(%c)",
    "!sparkType": "column"
  },
  "divide": {
    "!type": "fn(col1: Column, col2: Column) -> Column",
    "!doc": "Divides one number by another.",
    "!spark": "%c.divide(%c)",
    "!sparkType": "column"
  },
  "endsWith": {
    "!type": "fn(e: Column, other: Column) -> Column",
    "!doc": "String ends with.",
    "!spark": "%c.endsWith(%s)",
    "!sparkType": "column"
  },
  "equal": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Equality test.",
    "!spark": "%c.equalTo(%c)",
    "!sparkType": "column"
  },
  "equalNullSafe": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Equality test that is safe for null values.",
    "!spark": "%c.eqNullSafe(%c)",
    "!sparkType": "column"
  },
  "getField": {
    "!type": "fn(e: Column, fieldName: string) -> Column",
    "!doc": "Gets a field by name in a struct.",
    "!spark": "%c.getField(%s)",
    "!sparkType": "column"
  },
  "getItem": {
    "!type": "fn(e: Column, key: Column) -> Column",
    "!doc": "Gets at item out of an array or a map.",
    "!spark": "%c.getItem(%c)",
    "!sparkType": "column"
  },
  "greaterThan": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Greater than.",
    "!spark": "%c.gt(%c)",
    "!sparkType": "column"
  },
  "greaterThanOrEqual": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Greater than or equal to.",
    "!spark": "%c.geq(%c)",
    "!sparkType": "column"
  },
  "isin": {
    "!type": "fn(value: Column, list_varargs: Column) -> Column",
    "!doc":"Determines if the value is contained in the list.",
    "!spark": "%c.isin(%*c)",
    "!sparkType": "column"
  },
  "isNotNull": {
    "!type": "fn(e: Column) -> Column",
    "!doc": "True if the column is NOT null.",
    "!spark": "%c.isNotNull()",
    "!sparkType": "column"
  },
  "lessThan": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Less than.",
    "!spark": "%c.lt(%c)",
    "!sparkType": "column"
  },
  "lessThanOrEqual": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Less than or equal to.",
    "!spark": "%c.leq(%c)",
    "!sparkType": "column"
  },
  "like": {
    "!type": "fn(e: Column, literal: string) -> Column",
    "!doc": "SQL like expression.",
    "!spark": "%c.like(%s)",
    "!sparkType": "column"
  },
  "mod": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Remainder.",
    "!spark": "%c.mod(%c)",
    "!sparkType": "column"
  },
  "multiply": {
    "!type": "fn(col1: Column, col2: Column) -> Column",
    "!doc": "Multiplies one column by another.",
    "!spark": "%c.multiply(%c)",
    "!sparkType": "column"
  },
  "notEqual": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Inequality test.",
    "!spark": "%c.notEqual(%c)",
    "!sparkType": "column"
  },
  "or": {
    "!type": "fn(left: Column, right: Column) -> Column",
    "!doc": "Boolean OR.",
    "!spark": "%c.or(%c)",
    "!sparkType": "column"
  },
  "rlike": {
    "!type": "fn(e: Column, literal: string) -> Column",
    "!doc": "LIKE with Regex.",
    "!spark": "%c.rlike(%s)",
    "!sparkType": "column"
  },
  "startsWith": {
    "!type": "fn(e: Column, other: Column) -> Column",
    "!doc": "String starts with.",
    "!spark": "%c.startsWith(%c)",
    "!sparkType": "column"
  },
  "substr": {
    "!type": "fn(e: Column, startPos: Column, len: Column) -> Column",
    "!doc": "An expression that returns a substring.",
    "!spark": "%c.substr(%c, %c, %c)",
    "!sparkType": "column"
  },
  "subtract": {
    "!type": "fn(col1: Column, col2: Column) -> Column",
    "!doc": "Subtracts one number from another.",
    "!spark": "%c.minus(%c)",
    "!sparkType": "column"
  },

  "!DATA_FRAME_FUNCTIONS": "Functions on DataFrame objects.",

  "drop": {
    "!type": "fn(colName: string)",
    "!doc": "Drops the specified column.",
    "!spark": ".drop(%s)",
    "!sparkType": "dataframe"
  },
  "filter": {
    "!type": "fn(condition: Column)",
    "!doc": "Filters rows using the given condition.",
    "!spark": ".filter(%c)",
    "!sparkType": "dataframe"
  },
  "groupBy": {
    "!type": "fn(col1: Column, cols_varargs: Column) -> GroupedData",
    "!doc": "Groups using the specified columns.",
    "!spark": ".groupBy(%*c)",
    "!sparkType": "groupeddata"
  },
  "select": {
    "!type": "fn(cols_varargs: Column)",
    "!doc": "Selects a set of column based expressions.",
    "!spark": ".select(%*c)",
    "!sparkType": "dataframe"
  },

  "!HIVE_FUNCTIONS": "Functions available from Hive.",

  "xpath": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns a string array of values within xml nodes that match the xpath expression",
    "!spark": "functions.callUDF(\"xpath\", %c, %c)",
    "!sparkType": "column"
  },
  "xpath_double": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns a double value that matches the xpath expression",
    "!spark": "functions.callUDF(\"xpath_double\", %c, %c)",
    "!sparkType": "column"
  },
  "xpath_float": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns a float value that matches the xpath expression",
    "!spark": "functions.callUDF(\"xpath_float\", %c, %c)",
    "!sparkType": "column"
  },
  "xpath_int": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns an integer value that matches the xpath expression",
    "!spark": "functions.callUDF(\"xpath_int\", %c, %c)",
    "!sparkType": "column"
  },
  "xpath_long": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns a long value that matches the xpath expression",
    "!spark": "functions.callUDF(\"xpath_long\", %c, %c)",
    "!sparkType": "column"
  },
  "xpath_number": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns a double value that matches the xpath expression",
    "!spark": "functions.callUDF(\"xpath_number\", %c, %c)",
    "!sparkType": "column"
  },
  "xpath_short": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns a short value that matches the xpath expression",
    "!spark": "functions.callUDF(\"xpath_short\", %c, %c)",
    "!sparkType": "column"
  },
  "xpath_string": {
    "!type": "fn(xml: string, xpath: string) -> Column",
    "!doc": "Returns the text contents of the first xml node that matches the xpath expression",
    "!spark": "functions.callUDF(\"xpath_string\", %c, %c)",
    "!sparkType": "column"
  },

  "!MACHINE_LEARNING_FUNCTIONS": "Functions for performing machine learning operations on DataFrame objects.",

  "transform": {
    "!type": "fn(modelPath: string)",
    "!doc": "Transforms the dataset. (Spark 1.6)",
    "!spark": ".transform(org.apache.spark.ml.PipelineModel.load(%s).transform)",
    "!sparkType": "dataframe"
  },

  "!WINDOW_SPEC_FUNCTIONS": "Functions for creating WindowSpec objects.",

  "orderBy": {
    "!type": "fn(cols_varargs: Column) -> WindowSpec",
    "!doc": "Creates a WindowSpec with the ordering defined.",
    "!spark": "org.apache.spark.sql.expressions.Window.orderBy(%*c)",
    "!sparkType": "windowspec"
  },
  "partitionBy": {
    "!type": "fn(cols_varargs: Column) -> WindowSpec",
    "!doc": "Creates a WindowSpec with the partitioning defined.",
    "!spark": "org.apache.spark.sql.expressions.Window.partitionBy(%*c)",
    "!sparkType": "windowspec"
  }
}
